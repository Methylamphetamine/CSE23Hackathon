{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import pylab as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from process_data import process_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/shyamss/git_repos/CSE23Hackathon/process_data.py:9: DtypeWarning: Columns (10,14,20,22,35,36,37,38,39,40,41,42,43,44,46,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(filename)\n"
     ]
    }
   ],
   "source": [
    "# Returns the columnn names and the data\n",
    "cols, data = process_data(\"bld1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find indices at which the column names contain 'Zone Air Temperature'\n",
    "# we want to make these what we're predicting.\n",
    "# the rest is just inputs to our model\n",
    "idx_temperature = np.array(['Zone Air Temperature' in col for col in cols])\n",
    "X = data[cols[~idx_temperature]]\n",
    "y = data[cols[idx_temperature]]\n",
    "y1 = y.iloc[:,0:1]\n",
    "y2 = y.iloc[:,1:2]\n",
    "y3 = y.iloc[:,2:3]\n",
    "\n",
    "X = np.concatenate([X, y1, y2, y3], axis = 1)[:-1]\n",
    "y = np.concatenate([y1, y2, y3], axis = 1)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 20)\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "X_train = scaler_x.fit_transform(X_train)\n",
    "X_test = scaler_x.transform(X_test)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "y_test = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = jnp.array(X_train)\n",
    "X_test = jnp.array(X_test)\n",
    "y_train = jnp.array(y_train)\n",
    "y_test = jnp.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import matplotlib.pyplot as pl\n",
    "import itertools\n",
    "import jax\n",
    "import optax\n",
    "import jaxopt\n",
    "import pickle\n",
    "import h5py\n",
    "import warnings\n",
    "import wandb\n",
    "import process_data\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from jax import random, jit, grad, jacfwd, vmap\n",
    "from optax_adan import adan\n",
    "from jax.nn import sigmoid, gelu, relu, softmax, softplus\n",
    "from functools import partial\n",
    "from tqdm import trange\n",
    "from torch.utils import data\n",
    "from typing import Sequence, Callable, Any, Tuple\n",
    "from absl import logging\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "\n",
    "# from tueplots import bundles\n",
    "from jax.nn.initializers import glorot_normal, normal, zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "    activation: Callable\n",
    "\n",
    "    def setup(self):\n",
    "        # Regular Dense layer:\n",
    "        self.layers = [nn.Dense(feat, kernel_init = jax.nn.initializers.glorot_normal()) for feat in self.features]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        # Final Layer has no activation function applied:\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, key, X, y, batch_size):\n",
    "        self.key = key\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generate one batch of data\"\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        batch = self.__data_generation(subkey)\n",
    "        return batch\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        idx = random.choice(key, self.X.shape[0], shape=(self.batch_size,))\n",
    "        batch = (self.X[idx], self.y[idx])\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    def __init__(\n",
    "        self,\n",
    "        key,\n",
    "        layers,\n",
    "    ):\n",
    "        # Initialize parameters:\n",
    "        model1 = MLP(layers, jnp.sin)\n",
    "        model2 = MLP(layers, jnp.sin)\n",
    "        model3 = MLP(layers, jnp.sin)\n",
    "        \n",
    "        self.init1, self.apply1 = model1.init, model1.apply\n",
    "        self.init2, self.apply2 = model2.init, model2.apply\n",
    "        self.init3, self.apply3 = model3.init, model3.apply\n",
    "        \n",
    "        keys = random.split(key, 6)\n",
    "        params1 = self.init1(keys[0], random.normal(keys[1], (44,)))\n",
    "        params2 = self.init2(keys[2], random.normal(keys[3], (44,)))\n",
    "        params3 = self.init3(keys[4], random.normal(keys[5], (44,)))\n",
    "        \n",
    "        self.params = (params1, params2, params3)\n",
    "        self.avg_params = self.params\n",
    "        learning_schedule = optax.cosine_decay_schedule(1e-3, 200000, 1e-5)\n",
    "        self.optimizer = optax.adabelief(1e-3)\n",
    "        self.opt_state = self.optimizer.init(self.params)\n",
    "        self.loss_log = []        \n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss(self, params, batch):\n",
    "        params1, params2, params3 = params\n",
    "        X, y = batch\n",
    "        y1, y2, y3 = y[:, 0], y[:, 1], y[:, 2]\n",
    "        y1_pred = self.apply1(params1, X)\n",
    "        y2_pred = self.apply2(params2, X)\n",
    "        y3_pred = self.apply3(params3, X)\n",
    "        loss = jnp.mean((y1_pred[:, 0] - y1) ** 2 + (y2_pred[:, 0] - y2) ** 2 + (y3_pred[:, 0] - y3) ** 2)\n",
    "        return loss\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, params, opt_state, batch):\n",
    "        grad = jax.grad(self.loss)(params, batch)\n",
    "        updates, opt_state = self.optimizer.update(grad, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def ema_update(self, params, avg_params):\n",
    "        return optax.incremental_update(params, avg_params, step_size=0.001)\n",
    "    \n",
    "    def model_predict(self, params, X):\n",
    "        y1 = self.apply1(params[0], X)\n",
    "        y2 = self.apply2(params[1], X)\n",
    "        y3 = self.apply3(params[2], X)\n",
    "        return y1, y2, y3\n",
    "\n",
    "    # Optimize parameters in a loop\n",
    "    def train(self, dataloader, nIter=10000):\n",
    "        pbar = trange(nIter)\n",
    "        # Main training loop\n",
    "        for it in pbar:\n",
    "            batch = next(dataloader)\n",
    "            self.params, self.opt_state = self.step(self.params, self.opt_state, batch)\n",
    "            self.avg_params = self.ema_update(self.params, self.avg_params)\n",
    "            # Logger\n",
    "            if it % 1000 == 0:\n",
    "                params = self.avg_params\n",
    "                loss = self.loss(params, batch)\n",
    "                pbar.set_postfix({\"loss\": loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = iter(DataGenerator(random.PRNGKey(0), X_train, y_train, 1024))\n",
    "model = Regression(random.PRNGKey(42), [256, 256, 256, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [08:40<00:00, 959.74it/s, loss=0.00013228813] \n"
     ]
    }
   ],
   "source": [
    "model.train(loader, nIter=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.model_predict(model.avg_params, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 20)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_new = scaler.transform(X[:432])\n",
    "\n",
    "y_train = scaler.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot(y_new[0].ravel(), label = 'Predicted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "acte = r2_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(1.9073486e-06, dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(y_test_pred - y_test).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[  4.7566223,  -8.610168 , -13.243989 ],\n",
       "             [ 24.469007 ,  -5.446783 ,  -3.693832 ],\n",
       "             [ 33.19131  ,   8.277213 ,   4.131795 ],\n",
       "             ...,\n",
       "             [ 25.102198 ,   2.096973 ,  -1.5878501],\n",
       "             [ 24.972366 ,   6.1852317,   4.4039264],\n",
       "             [ 22.779865 ,  22.935507 ,  40.540382 ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuttingedge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4de6a45db83da0b758db7d08be5de2783b8b59abffac295330c3911b6a615d7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
